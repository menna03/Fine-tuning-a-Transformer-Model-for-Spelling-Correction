{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11762848,"sourceType":"datasetVersion","datasetId":7384614}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- Environment Setup ---\n!pip install transformers datasets evaluate jiwer sentencepiece accelerate -q\n!pip show transformers datasets evaluate jiwer # Optional: check versions","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:07:41.520662Z","iopub.execute_input":"2025-05-14T00:07:41.521012Z","iopub.status.idle":"2025-05-14T00:09:57.031430Z","shell.execute_reply.started":"2025-05-14T00:07:41.520988Z","shell.execute_reply":"2025-05-14T00:09:57.029409Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:02\u001b[0mmm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mName: transformers\nVersion: 4.51.3\nSummary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\nHome-page: https://github.com/huggingface/transformers\nAuthor: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\nAuthor-email: transformers@huggingface.co\nLicense: Apache 2.0 License\nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\nRequired-by: kaggle-environments, peft, sentence-transformers\n---\nName: datasets\nVersion: 3.6.0\nSummary: HuggingFace community-driven open-source library of datasets\nHome-page: https://github.com/huggingface/datasets\nAuthor: HuggingFace Inc.\nAuthor-email: thomas@huggingface.co\nLicense: Apache 2.0\nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, tqdm, xxhash\nRequired-by: evaluate, torchtune\n---\nName: evaluate\nVersion: 0.4.3\nSummary: HuggingFace community-driven open-source library of evaluation\nHome-page: https://github.com/huggingface/evaluate\nAuthor: HuggingFace Inc.\nAuthor-email: leandro@huggingface.co\nLicense: Apache 2.0\nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: datasets, dill, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, requests, tqdm, xxhash\nRequired-by: \n---\nName: jiwer\nVersion: 3.1.0\nSummary: Evaluate your speech-to-text system with similarity measures such as word error rate (WER)\nHome-page: \nAuthor: \nAuthor-email: Nik Vaessen <nikvaes@gmail.com>\nLicense: \nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: click, rapidfuzz\nRequired-by: \n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# --- Imports ---\nimport os\nimport random\nimport re\nimport gc # For garbage collection\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom datasets import Dataset as HFDataset\nfrom nltk.metrics.distance import edit_distance\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm # For progress bars\n\nfrom transformers import (\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq,\n    EarlyStoppingCallback\n)\nimport evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:09:57.034400Z","iopub.execute_input":"2025-05-14T00:09:57.034789Z","iopub.status.idle":"2025-05-14T00:09:57.558083Z","shell.execute_reply.started":"2025-05-14T00:09:57.034753Z","shell.execute_reply":"2025-05-14T00:09:57.556822Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept nltk.downloader.DownloadError:\n    print(\"Downloading NLTK punkt tokenizer...\")\n    nltk.download('punkt', quiet=True)\n    print(\"NLTK punkt tokenizer downloaded.\")\n\n# Set random seeds for reproducibility\ndef set_seed(seed_value=42):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed_value)\n    print(f\"Seed set to {seed_value}\")\n\nset_seed(42)\n\n# Define paths (CRITICAL: ADJUST DATA_DIR)\nDATA_DIR = \"/kaggle/input/transformer-mode-for-spelling-correction/wiki-split-master\"\n\n\nCACHE_DIR = \"/kaggle/working/cache_dir_spell_correction\"\nOUTPUT_DIR = \"/kaggle/working/spell_correction_outputs\"\n\nos.makedirs(CACHE_DIR, exist_ok=True)\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"Cache directory: {os.path.abspath(CACHE_DIR)}\")\nprint(f\"Output directory: {os.path.abspath(OUTPUT_DIR)}\")\n\n\n# Model Configuration\nMODEL_NAME = \"t5-small\"\nprint(f\"Using model: {MODEL_NAME}\")\n\n# Dataset Configuration (ADJUST THESE FOR YOUR RUN)\n# Number of ORIGINAL correct sentences to load from WikiSplit\n# For a quick test:\nMAX_TRAIN_SAMPLES_ORIGINAL = 5000  # Reduced for quick testing\nMAX_VAL_SAMPLES_ORIGINAL = 500    # Reduced\nMAX_TEST_SAMPLES_ORIGINAL = 500   # Reduced\n# For a more serious run, increase these significantly (e.g., 50000, 5000, 5000)\n\nVARIANTS_PER_SENTENCE = 1 # Number of misspelled versions per correct sentence\nprint(f\"Max original train samples: {MAX_TRAIN_SAMPLES_ORIGINAL}, Validation: {MAX_VAL_SAMPLES_ORIGINAL}, Test: {MAX_TEST_SAMPLES_ORIGINAL}\")\nprint(f\"Misspelled variants per sentence: {VARIANTS_PER_SENTENCE}\")\n\n# Tokenizer Configuration\nMAX_SOURCE_LENGTH = 128 # Max length for misspelled input\nMAX_TARGET_LENGTH = 128 # Max length for corrected output\nprint(f\"Max source length: {MAX_SOURCE_LENGTH}, Max target length: {MAX_TARGET_LENGTH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:13:24.207493Z","iopub.execute_input":"2025-05-14T00:13:24.207820Z","iopub.status.idle":"2025-05-14T00:13:24.221037Z","shell.execute_reply.started":"2025-05-14T00:13:24.207799Z","shell.execute_reply":"2025-05-14T00:13:24.219967Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nSeed set to 42\nCache directory: /kaggle/working/cache_dir_spell_correction\nOutput directory: /kaggle/working/spell_correction_outputs\nUsing model: t5-small\nMax original train samples: 5000, Validation: 500, Test: 500\nMisspelled variants per sentence: 1\nMax source length: 128, Max target length: 128\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# --- 1. Load the Sentences (from WikiSplit) ---\n\ndef load_wikisplit_data(file_path, max_samples=None, file_label=\"data\"):\n    \n    print(f\"Loading {file_label} from {file_path}...\")\n    try:\n        df = pd.read_csv(file_path, sep='\\t', header=None, names=['correct', 'simple'], on_bad_lines='warn')\n        df = df[['correct']] # Keep only the 'correct' column\n        df.dropna(subset=['correct'], inplace=True) # Remove rows where 'correct' is NaN\n        df['correct'] = df['correct'].astype(str).str.strip() # Ensure string type and strip whitespace\n        df = df[df['correct'] != ''] # Remove empty strings\n    except FileNotFoundError:\n        print(f\"ERROR: File not found at {file_path}. Please ensure DATA_DIR is correct and file exists.\")\n        return pd.DataFrame(columns=['correct'])\n    except Exception as e:\n        print(f\"An error occurred while loading {file_path}: {e}\")\n        return pd.DataFrame(columns=['correct'])\n\n    if df.empty:\n        print(f\"Warning: No data loaded from {file_path}. The DataFrame is empty.\")\n        return df\n\n    if max_samples and len(df) > max_samples:\n        print(f\"Sampling {max_samples} from {len(df)} available sentences.\")\n        df = df.sample(n=max_samples, random_state=42) # Use fixed random_state for reproducibility\n\n    print(f\"Successfully loaded {len(df)} samples from {file_path}.\")\n    return df\n\n# Load original correct sentences from WikiSplit files\nprint(\"\\nLoading original correct sentences from WikiSplit files...\")\ntrain_df_orig = load_wikisplit_data(os.path.join(DATA_DIR, \"train.tsv\"), MAX_TRAIN_SAMPLES_ORIGINAL, \"training data\")\nval_df_orig = load_wikisplit_data(os.path.join(DATA_DIR, \"validation.tsv\"), MAX_VAL_SAMPLES_ORIGINAL, \"validation data\")\ntest_df_orig = load_wikisplit_data(os.path.join(DATA_DIR, \"test.tsv\"), MAX_TEST_SAMPLES_ORIGINAL, \"test data\")\n\n# Basic check to ensure data was loaded\nif train_df_orig.empty:\n    raise ValueError(\n        \"Training data (train_df_orig) is empty. \"\n        \"Please check your DATA_DIR path and ensure 'train.tsv' exists and is readable. \"\n        \"Current DATA_DIR: \" + os.path.abspath(DATA_DIR)\n    )\nif val_df_orig.empty:\n    print(\"Warning: Validation data (val_df_orig) is empty. Validation during training will be skipped or fail.\")\nif test_df_orig.empty:\n    print(\"Warning: Test data (test_df_orig) is empty. Final evaluation on test set will be skipped.\")\n\nprint(f\"\\nNumber of original correct sentences loaded:\")\nprint(f\"  Train: {len(train_df_orig)}\")\nprint(f\"  Validation: {len(val_df_orig)}\")\nprint(f\"  Test: {len(test_df_orig)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:13:24.600444Z","iopub.execute_input":"2025-05-14T00:13:24.600774Z","iopub.status.idle":"2025-05-14T00:13:32.232467Z","shell.execute_reply.started":"2025-05-14T00:13:24.600754Z","shell.execute_reply":"2025-05-14T00:13:32.231308Z"}},"outputs":[{"name":"stdout","text":"\nLoading original correct sentences from WikiSplit files...\nLoading training data from /kaggle/input/transformer-mode-for-spelling-correction/wiki-split-master/train.tsv...\nSampling 5000 from 989944 available sentences.\nSuccessfully loaded 5000 samples from /kaggle/input/transformer-mode-for-spelling-correction/wiki-split-master/train.tsv.\nLoading validation data from /kaggle/input/transformer-mode-for-spelling-correction/wiki-split-master/validation.tsv...\nSampling 500 from 5000 available sentences.\nSuccessfully loaded 500 samples from /kaggle/input/transformer-mode-for-spelling-correction/wiki-split-master/validation.tsv.\nLoading test data from /kaggle/input/transformer-mode-for-spelling-correction/wiki-split-master/test.tsv...\nSampling 500 from 5000 available sentences.\nSuccessfully loaded 500 samples from /kaggle/input/transformer-mode-for-spelling-correction/wiki-split-master/test.tsv.\n\nNumber of original correct sentences loaded:\n  Train: 5000\n  Validation: 500\n  Test: 500\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# --- 2. Generate Sentences with Spelling Mistakes (Creative Approach) ---\n\ndef generate_misspelled_sentence(sentence, error_prob_word=0.25, char_error_types=None):\n    \"\"\"\n    Introduces various character-level spelling errors into a sentence.\n    error_prob_word: Probability that a word will have an error introduced.\n    \"\"\"\n    if char_error_types is None:\n        # Common character error types\n        char_error_types = ['swap', 'delete', 'insert', 'replace', 'duplicate']\n    \n    words = sentence.split(' ') # Simple split, could use nltk.word_tokenize for more robustness\n    misspelled_words = []\n    alphabet = \"abcdefghijklmnopqrstuvwxyz\" # For insertions/replacements\n\n    for word in words:\n        # Only attempt to misspell words with alphabetic characters and some length\n        if random.random() < error_prob_word and len(word) > 1 and any(c.isalpha() for c in word):\n            chars = list(word)\n            chosen_error_type = random.choice(char_error_types)\n            \n            can_apply_error = False\n            if chosen_error_type == 'swap' and len(chars) >= 2:\n                idx = random.randint(0, len(chars) - 2)\n                chars[idx], chars[idx+1] = chars[idx+1], chars[idx]\n                can_apply_error = True\n            elif chosen_error_type == 'delete' and len(chars) >= 2: # Avoid deleting the only char\n                idx_to_delete = random.randint(0, len(chars) - 1)\n                chars.pop(idx_to_delete)\n                can_apply_error = True\n            elif chosen_error_type == 'insert':\n                idx_to_insert = random.randint(0, len(chars))\n                chars.insert(idx_to_insert, random.choice(alphabet))\n                can_apply_error = True\n            elif chosen_error_type == 'replace' and len(chars) >= 1:\n                idx_to_replace = random.randint(0, len(chars) - 1)\n                original_char = chars[idx_to_replace]\n                if original_char.isalpha(): # Only replace alphabetic chars meaningfully\n                    new_char = random.choice(alphabet)\n                    while new_char == original_char.lower() and len(alphabet) > 1: # Avoid replacing with same char\n                        new_char = random.choice(alphabet)\n                    chars[idx_to_replace] = new_char if original_char.islower() else new_char.upper()\n                    can_apply_error = True\n            elif chosen_error_type == 'duplicate' and len(chars) >= 1:\n                idx_to_duplicate = random.randint(0, len(chars) - 1)\n                chars.insert(idx_to_duplicate, chars[idx_to_duplicate]) # Duplicate the char at idx\n                can_apply_error = True\n            \n            if can_apply_error:\n                misspelled_words.append(\"\".join(chars))\n            else: # If error couldn't be applied (e.g., word too short for swap/delete)\n                misspelled_words.append(word)\n        else:\n            misspelled_words.append(word)\n            \n    return \" \".join(misspelled_words)\n\n# Test the generator\ntest_sentence = \"This is a sample sentence for testing the error generator.\"\nprint(f\"Original test sentence: {test_sentence}\")\nfor i in range(3):\n    print(f\"Misspelled variant {i+1}: {generate_misspelled_sentence(test_sentence)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:13:32.234355Z","iopub.execute_input":"2025-05-14T00:13:32.234779Z","iopub.status.idle":"2025-05-14T00:13:32.249354Z","shell.execute_reply.started":"2025-05-14T00:13:32.234753Z","shell.execute_reply":"2025-05-14T00:13:32.248140Z"}},"outputs":[{"name":"stdout","text":"Original test sentence: This is a sample sentence for testing the error generator.\nMisspelled variant 1: This his a ssample sentence ofr ttesting the error generator.\nMisspelled variant 2: This is a sample sentence fir tcesting the error generator.\nMisspelled variant 3: This ms a sample sentence for testing hte error generator.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# --- 3. Create Train, Validation, and Test Datasets with Misspelled Pairs ---\n\ndef create_spelling_dataset_from_correct(df_correct, num_variants_per_sentence=1, dataset_label=\"data\"):\n    \"\"\"\n    Generates misspelled versions for each correct sentence in the DataFrame.\n    Returns a DataFrame with 'misspelled' and 'correct' columns.\n    \"\"\"\n    data_pairs = []\n    if df_correct.empty:\n        print(f\"Warning: Input DataFrame for {dataset_label} is empty. No misspelled variants will be generated.\")\n        return pd.DataFrame(columns=['misspelled', 'correct'])\n\n    print(f\"\\nGenerating {num_variants_per_sentence} misspelled variant(s) per sentence for {dataset_label}...\")\n    for _, row in tqdm(df_correct.iterrows(), total=len(df_correct), desc=f\"Generating for {dataset_label}\"):\n        correct_sentence = row['correct']\n        \n        # Basic filter: ensure sentence is a string and not excessively long\n        # (MAX_SOURCE_LENGTH - 10 reserves tokens for the \"correct spelling: \" prefix)\n        if not isinstance(correct_sentence, str) or len(correct_sentence.split()) > (MAX_SOURCE_LENGTH - 10):\n            continue # Skip this sentence\n\n        for _ in range(num_variants_per_sentence):\n            misspelled = generate_misspelled_sentence(correct_sentence)\n            # Ensure the generated misspelled sentence is actually different and not empty\n            if misspelled != correct_sentence and misspelled.strip():\n                data_pairs.append({'misspelled': misspelled, 'correct': correct_sentence})\n            # else:\n                # Optional: Log if no valid misspelling was generated for a sentence\n                # print(f\"Could not generate a distinct non-empty misspelling for: {correct_sentence}\")\n                \n    return pd.DataFrame(data_pairs)\n\n# Generate misspelled datasets\ntrain_data_df = create_spelling_dataset_from_correct(train_df_orig, VARIANTS_PER_SENTENCE, \"training\")\nval_data_df = create_spelling_dataset_from_correct(val_df_orig, VARIANTS_PER_SENTENCE, \"validation\")\ntest_data_df = create_spelling_dataset_from_correct(test_df_orig, VARIANTS_PER_SENTENCE, \"test\") # Keep for final demo\n\nprint(f\"\\nNumber of generated misspelled/correct pairs:\")\nprint(f\"  Train: {len(train_data_df)}\")\nprint(f\"  Validation: {len(val_data_df)}\")\nprint(f\"  Test: {len(test_data_df)}\")\n\n\n# Display a few examples from the generated training data\nif not train_data_df.empty:\n    print(\"\\nSample generated training data (first 3 pairs):\")\n    for i in range(min(3, len(train_data_df))):\n        print(f\"  Correct   : {train_data_df.iloc[i]['correct']}\")\n        print(f\"  Misspelled: {train_data_df.iloc[i]['misspelled']}\")\n        print(\"-\" * 30)\nelse:\n    print(\"Warning: No training data (train_data_df) was generated. Training will likely fail or be skipped.\")\n    # Potentially raise an error if training data is essential and empty\n    # raise ValueError(\"Training data generation resulted in an empty dataset.\")\n\n# Convert pandas DataFrames to Hugging Face Dataset objects\nif not train_data_df.empty:\n    train_dataset_hf = HFDataset.from_pandas(train_data_df)\nelse:\n    train_dataset_hf = HFDataset.from_dict({'misspelled': [], 'correct': []}) # Empty HF dataset\n\nif not val_data_df.empty:\n    val_dataset_hf = HFDataset.from_pandas(val_data_df)\nelse:\n    val_dataset_hf = HFDataset.from_dict({'misspelled': [], 'correct': []})\n\nif not test_data_df.empty:\n    test_dataset_hf_for_tokenization = HFDataset.from_pandas(test_data_df) # For tokenization\nelse:\n    test_dataset_hf_for_tokenization = HFDataset.from_dict({'misspelled': [], 'correct': []})\n# Note: test_data_df (pandas) is kept for the final string-based demonstration\n\n# Clean up original DataFrames to save memory (if they are large)\ndel train_df_orig, val_df_orig, test_df_orig\ngc.collect()\n\nprint(f\"\\nCreated Hugging Face Datasets:\")\nprint(f\"  Train: {train_dataset_hf}\")\nprint(f\"  Validation: {val_dataset_hf}\")\nprint(f\"  Test (for tokenization): {test_dataset_hf_for_tokenization}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:13:32.250439Z","iopub.execute_input":"2025-05-14T00:13:32.250798Z","iopub.status.idle":"2025-05-14T00:13:33.447946Z","shell.execute_reply.started":"2025-05-14T00:13:32.250768Z","shell.execute_reply":"2025-05-14T00:13:33.446676Z"}},"outputs":[{"name":"stdout","text":"\nGenerating 1 misspelled variant(s) per sentence for training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating for training:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6447f0bebdb6494b9c63935406bcb9fd"}},"metadata":{}},{"name":"stdout","text":"\nGenerating 1 misspelled variant(s) per sentence for validation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating for validation:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3743872338864ccd92b20ece17182949"}},"metadata":{}},{"name":"stdout","text":"\nGenerating 1 misspelled variant(s) per sentence for test...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating for test:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d847197f239844c1b8ef1ce1433adc4e"}},"metadata":{}},{"name":"stdout","text":"\nNumber of generated misspelled/correct pairs:\n  Train: 4987\n  Validation: 497\n  Test: 500\n\nSample generated training data (first 3 pairs):\n  Correct   : After the very heavy '' Dragontown '' album Alice decided to return to his roots , his place of birth , Detroit , where he accidently joined in with a festival together with MC5 & Iggy and the Stooges .\n  Misspelled: After the very heavy '' Dragontown '' album Alice decided to retun to hus roots , his place fo birth , Detroit , where hu accidently joined inn with a festival together with YC5 & Igg and the Stooges .\n------------------------------\n  Correct   : The museum runs a library with photographic books and magazines , and a small museum store that sells postcards , posters and more .\n  Misspelled: The museum runs a libraryy with photographic baoks and magazines , and a small museum store that sells postcard , posters and more .\n------------------------------\n  Correct   : Jakobshavn Isbræ is a major contributor to the mass balance of the Greenland ice sheet , producing some 10 % of all Greenland icebergs some 35 billion tonnes of icebergs calved off and passing out of the fjord every year .\n  Misspelled: Jakohbshavn IIsbræ ys a majof contributor to the maass balance of the Greenland ice sheet , proudcing some 10 % fo all Greeland icebergs som 35 billion tonnes on icebergs calved fof and passing out o te fjord every yyear .\n------------------------------\n\nCreated Hugging Face Datasets:\n  Train: Dataset({\n    features: ['misspelled', 'correct'],\n    num_rows: 4987\n})\n  Validation: Dataset({\n    features: ['misspelled', 'correct'],\n    num_rows: 497\n})\n  Test (for tokenization): Dataset({\n    features: ['misspelled', 'correct'],\n    num_rows: 500\n})\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# --- 4. Perform Necessary Data Preparation and Preprocessing ---\n\n# Initialize tokenizer\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\nprint(f\"Tokenizer loaded: {MODEL_NAME}\")\nprint(f\"  Vocabulary size: {tokenizer.vocab_size}\")\nprint(f\"  Pad token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\nprint(f\"  EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n\n\ndef preprocess_function(examples):\n    \"\"\"Tokenizes misspelled inputs and correct target sentences for T5.\"\"\"\n    # For T5, a task-specific prefix is often beneficial.\n    inputs = [\"correct spelling: \" + doc for doc in examples[\"misspelled\"]]\n    \n    # Tokenize the inputs (misspelled sentences)\n    model_inputs = tokenizer(\n        inputs, \n        max_length=MAX_SOURCE_LENGTH, \n        padding=\"max_length\", # Pad to max_length\n        truncation=True       # Truncate if longer than max_length\n    )\n\n    # Tokenize the targets (correct sentences)\n    # The 'with tokenizer.as_target_tokenizer():' context manager ensures\n    # that the tokenizer prepares the labels suitable for T5's decoder.\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"correct\"], \n            max_length=MAX_TARGET_LENGTH, \n            padding=\"max_length\", # Pad to max_length\n            truncation=True       # Truncate if longer\n        )\n\n    # The model expects the target token IDs in the 'labels' field.\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nprint(\"\\nTokenizing datasets...\")\nif len(train_dataset_hf) > 0:\n    tokenized_train_dataset = train_dataset_hf.map(\n        preprocess_function,\n        batched=True,\n        remove_columns=[\"misspelled\", \"correct\"],\n        desc=\"Preprocessing train dataset\"\n    )\n    print(f\"Tokenized train dataset: {tokenized_train_dataset}\")\nelse:\n    tokenized_train_dataset = train_dataset_hf # Keep as empty if source was empty\n    print(\"Skipping tokenization for empty train dataset.\")\n\nif len(val_dataset_hf) > 0:\n    tokenized_val_dataset = val_dataset_hf.map(\n        preprocess_function,\n        batched=True,\n        remove_columns=[\"misspelled\", \"correct\"],\n        desc=\"Preprocessing validation dataset\"\n    )\n    print(f\"Tokenized validation dataset: {tokenized_val_dataset}\")\nelse:\n    tokenized_val_dataset = val_dataset_hf\n    print(\"Skipping tokenization for empty validation dataset.\")\n    \nif len(test_dataset_hf_for_tokenization) > 0:\n    tokenized_test_dataset_for_eval = test_dataset_hf_for_tokenization.map(\n        preprocess_function,\n        batched=True,\n        remove_columns=[\"misspelled\", \"correct\"],\n        desc=\"Preprocessing test dataset for evaluation\"\n    )\n    print(f\"Tokenized test dataset (for eval): {tokenized_test_dataset_for_eval}\")\nelse:\n    tokenized_test_dataset_for_eval = test_dataset_hf_for_tokenization\n    print(\"Skipping tokenization for empty test dataset.\")\n\n# Verify an example from the tokenized training set\nif len(tokenized_train_dataset) > 0:\n    print(\"\\nExample of a tokenized training sample:\")\n    sample = tokenized_train_dataset[0]\n    print(f\"  Input IDs: {sample['input_ids'][:20]}... (len: {len(sample['input_ids'])})\")\n    print(f\"  Decoded Input: {tokenizer.decode(sample['input_ids'], skip_special_tokens=False)}\")\n    print(f\"  Labels: {sample['labels'][:20]}... (len: {len(sample['labels'])})\")\n    print(f\"  Decoded Labels: {tokenizer.decode(sample['labels'], skip_special_tokens=False)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:13:33.450248Z","iopub.execute_input":"2025-05-14T00:13:33.450537Z","iopub.status.idle":"2025-05-14T00:13:37.617723Z","shell.execute_reply.started":"2025-05-14T00:13:33.450516Z","shell.execute_reply":"2025-05-14T00:13:37.616718Z"}},"outputs":[{"name":"stdout","text":"Tokenizer loaded: t5-small\n  Vocabulary size: 32000\n  Pad token: '<pad>' (ID: 0)\n  EOS token: '</s>' (ID: 1)\n\nTokenizing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Preprocessing train dataset:   0%|          | 0/4987 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a13dc5bd1644494fa588e129fb637cba"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Tokenized train dataset: Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 4987\n})\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Preprocessing validation dataset:   0%|          | 0/497 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac59963bc1b64ae3bc910a9f615a1106"}},"metadata":{}},{"name":"stdout","text":"Tokenized validation dataset: Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 497\n})\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Preprocessing test dataset for evaluation:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55ae807740eb4b599810ceecb44665ee"}},"metadata":{}},{"name":"stdout","text":"Tokenized test dataset (for eval): Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 500\n})\n\nExample of a tokenized training sample:\n  Input IDs: [2024, 19590, 10, 621, 8, 182, 2437, 3, 31, 31, 10282, 3540, 3, 31, 31, 2306, 13390, 1500, 12, 3]... (len: 128)\n  Decoded Input: correct spelling: After the very heavy '' Dragontown '' album Alice decided to retun to hus roots, his place fo birth, Detroit, where hu accidently joined inn with a festival together with YC5 & Igg and the Stooges.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n  Labels: [621, 8, 182, 2437, 3, 31, 31, 10282, 3540, 3, 31, 31, 2306, 13390, 1500, 12, 1205, 12, 112, 8523]... (len: 128)\n  Decoded Labels: After the very heavy '' Dragontown '' album Alice decided to return to his roots, his place of birth, Detroit, where he accidently joined in with a festival together with MC5 & Iggy and the Stooges.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# --- 5. Choose Transformer (T5), Prepare for Fine-tuning ---\n# --- 6. Methods for Overfitting (Callbacks, Weight Decay) ---\n# --- 7. Evaluation Metrics Definition ---\n\n\n\n\n# Define Evaluation Metrics (WER and CER)\nwer_metric = evaluate.load(\"wer\")\ncer_metric = evaluate.load(\"cer\")\nprint(\"WER and CER metrics loaded from 'evaluate' library.\")\n\n# In Cell 6 (Model Loading, Metrics, and Training Arguments)\n# Modify your compute_metrics function\n\ndef compute_metrics(eval_preds):\n    \"\"\"Computes WER, CER, and Exact Match Accuracy for evaluation.\"\"\"\n    preds_outputs, labels_ids = eval_preds # eval_preds is a tuple (predictions, labels)\n    \n    if isinstance(preds_outputs, tuple):\n        preds_ids = preds_outputs[0]\n    else:\n        preds_ids = preds_outputs\n\n    # --- TEMP DEBUGGING: PRINT RAW PREDICTED TOKEN IDs ---\n    print(f\"\\n[DEBUG compute_metrics] Raw preds_ids (first example, up to 50 tokens): {preds_ids[0, :50] if preds_ids.ndim > 1 and preds_ids.shape[0] > 0 else preds_ids[:50]}\")\n    print(f\"[DEBUG compute_metrics] Shape of preds_ids: {preds_ids.shape}\")\n    print(f\"[DEBUG compute_metrics] Min ID in preds: {np.min(preds_ids)}, Max ID in preds: {np.max(preds_ids)}\")\n    print(f\"[DEBUG compute_metrics] Tokenizer vocab size: {tokenizer.vocab_size}\")\n    # --- END TEMP DEBUGGING ---\n\n    # It's possible preds_ids are already a numpy array, but ensure it for np.where\n    if isinstance(preds_ids, torch.Tensor):\n        preds_ids_np = preds_ids.cpu().numpy()\n    else:\n        preds_ids_np = np.array(preds_ids)\n\n\n    # Filter out any potential out-of-bounds IDs *before* decoding as a safeguard,\n    # though the root cause needs to be fixed.\n    # This is a temporary workaround to PREVENT the crash for diagnosis, not a fix.\n    vocab_size = tokenizer.vocab_size\n    preds_ids_np_clipped = np.clip(preds_ids_np, 0, vocab_size - 1)\n\n\n    labels_ids_np = np.array(labels_ids) # Ensure labels_ids is also a numpy array\n    labels_ids_np = np.where(labels_ids_np != -100, labels_ids_np, tokenizer.pad_token_id)\n    \n    # Use the clipped IDs for decoding\n    try:\n        # decoded_preds = tokenizer.batch_decode(preds_ids, skip_special_tokens=True) # Original\n        decoded_preds = tokenizer.batch_decode(preds_ids_np_clipped, skip_special_tokens=True) # Using clipped\n        decoded_labels = tokenizer.batch_decode(labels_ids_np, skip_special_tokens=True)\n    except IndexError as e:\n        print(f\"IndexError during batch_decode even after clipping: {e}\")\n        print(f\"Problematic preds_ids sample (clipped): {preds_ids_np_clipped[0, :50] if preds_ids_np_clipped.ndim > 1 and preds_ids_np_clipped.shape[0] > 0 else preds_ids_np_clipped[:50]}\")\n        # Fallback if decoding still fails\n        decoded_preds = [\" [DECODING_ERROR] \" for _ in range(preds_ids_np_clipped.shape[0])]\n        decoded_labels = tokenizer.batch_decode(labels_ids_np, skip_special_tokens=True)\n\n\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [label.strip() for label in decoded_labels]\n    \n    # ... (rest of your metric calculations: wer, cer, accuracy) ...\n    try:\n        wer = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n        wer_score = wer['wer'] if isinstance(wer, dict) and 'wer' in wer else wer \n    except Exception as e:\n        print(f\"Warning: Could not compute WER: {e}. Returning high value.\")\n        wer_score = 1.0\n    \n    try:\n        cer = cer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n        cer_score = cer['cer'] if isinstance(cer, dict) and 'cer' in cer else cer\n    except Exception as e:\n        print(f\"Warning: Could not compute CER: {e}. Returning high value.\")\n        cer_score = 1.0\n    \n    exact_matches = sum(1 for pred, label in zip(decoded_preds, decoded_labels) if pred == label)\n    accuracy = exact_matches / len(decoded_preds) if len(decoded_preds) > 0 else 0.0\n    \n    return {\n        \"wer\": wer_score if wer_score is not None else 1.0,\n        \"cer\": cer_score if cer_score is not None else 1.0,\n        \"exact_match_accuracy\": accuracy\n    }\n\n# Training Arguments\n\neffective_batch_size = 10\nper_device_train_bs = 4 \nper_device_eval_bs = per_device_train_bs * 2 \n\nif per_device_train_bs == 0: \n    per_device_train_bs = 1\ngradient_accumulation_steps = max(1, effective_batch_size // per_device_train_bs)\n\nprint(f\"\\nTraining Configuration:\")\nprint(f\"  Per-device train batch size: {per_device_train_bs}\")\nprint(f\"  Per-device eval batch size: {per_device_eval_bs}\")\nprint(f\"  Gradient accumulation steps: {gradient_accumulation_steps}\")\nprint(f\"  Effective train batch size: {per_device_train_bs * gradient_accumulation_steps}\")\n\n# Ensure Seq2SeqTrainingArguments is imported\nfrom transformers import Seq2SeqTrainingArguments \nimport os # for os.path.join\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    eval_strategy=\"steps\",\n    eval_steps=250,\n    save_strategy=\"steps\",\n    save_steps=250,\n    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"), \n    logging_steps=50,\n    learning_rate=5e-5,\n    per_device_train_batch_size=per_device_train_bs,\n    per_device_eval_batch_size=per_device_eval_bs,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=2,\n    predict_with_generate=True,\n    generation_max_length=MAX_TARGET_LENGTH, \n    fp16=torch.cuda.is_available(),\n    load_best_model_at_end=True,\n    metric_for_best_model=\"cer\",\n    greater_is_better=False,\n    report_to=\"none\",\n    dataloader_num_workers=0,\n)\n\n# Data Collator\n# Ensure DataCollatorForSeq2Seq is imported\nfrom transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model, \n    label_pad_token_id=-100,\n    pad_to_multiple_of=8 if training_args.fp16 else None \n)\nprint(\"TrainingArguments and DataCollator initialized.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:47:51.869457Z","iopub.execute_input":"2025-05-14T00:47:51.870670Z","iopub.status.idle":"2025-05-14T00:47:52.490272Z","shell.execute_reply.started":"2025-05-14T00:47:51.870632Z","shell.execute_reply":"2025-05-14T00:47:52.488980Z"}},"outputs":[{"name":"stdout","text":"WER and CER metrics loaded from 'evaluate' library.\n\nTraining Configuration:\n  Per-device train batch size: 4\n  Per-device eval batch size: 8\n  Gradient accumulation steps: 2\n  Effective train batch size: 8\nTrainingArguments and DataCollator initialized.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# --- Initialize Trainer ---\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset if len(tokenized_train_dataset) > 0 else None,\n    eval_dataset=tokenized_val_dataset if len(tokenized_val_dataset) > 0 else None,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop if metric doesn't improve for 3 evals\n)\nprint(\"Seq2SeqTrainer initialized.\")\n\n# --- Start Training (Requirement 5 cont.) ---\nprint(\"\\nStarting model fine-tuning...\")\n# Check if datasets are valid for training\ncan_train = False\nif tokenized_train_dataset and len(tokenized_train_dataset) > 0:\n    if tokenized_val_dataset and len(tokenized_val_dataset) > 0:\n        can_train = True\n        print(f\"Training with {len(tokenized_train_dataset)} train samples and {len(tokenized_val_dataset)} validation samples.\")\n    else:\n        print(\"Warning: Validation dataset is empty. Training without validation is not recommended for finding the best model.\")\nelse:\n    print(\"Training dataset is empty. Skipping training.\")\n\nif can_train:\n    try:\n        print(\"Calling trainer.train()...\")\n        train_result = trainer.train()\n        print(\"Training finished successfully.\")\n        \n        # Save training metrics\n        trainer.log_metrics(\"train\", train_result.metrics)\n        trainer.save_metrics(\"train\", train_result.metrics)\n        trainer.save_state() # Saves optimizer, scheduler, etc.\n\n        # Save the best model explicitly (though load_best_model_at_end should handle it)\n        best_model_path = os.path.join(OUTPUT_DIR, \"best_spell_corrector_model\")\n        trainer.save_model(best_model_path)\n        tokenizer.save_pretrained(best_model_path) # Save tokenizer with the model\n        print(f\"Best model and tokenizer saved to {best_model_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during training: {e}\")\n        import traceback\n        traceback.print_exc()\nelse:\n    if not (tokenized_train_dataset and len(tokenized_train_dataset) > 0) :\n        print(\"Skipping training because the training dataset is empty or invalid.\")\n    elif not (tokenized_val_dataset and len(tokenized_val_dataset) > 0) :\n         print(\"Skipping training because the validation dataset is empty and training_args expect validation.\")\n\n\n# Clean up GPU memory after training\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(\"GPU memory cache cleared after training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:47:53.424666Z","iopub.execute_input":"2025-05-14T00:47:53.425065Z","iopub.status.idle":"2025-05-14T03:02:28.367054Z","shell.execute_reply.started":"2025-05-14T00:47:53.425040Z","shell.execute_reply":"2025-05-14T03:02:28.365692Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/1036528892.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Seq2SeqTrainer initialized.\n\nStarting model fine-tuning...\nTraining with 4987 train samples and 497 validation samples.\nCalling trainer.train()...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1246' max='1246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1246/1246 2:14:26, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Wer</th>\n      <th>Cer</th>\n      <th>Exact Match Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>250</td>\n      <td>0.214000</td>\n      <td>0.188294</td>\n      <td>0.162240</td>\n      <td>0.077486</td>\n      <td>0.026157</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.226800</td>\n      <td>0.175814</td>\n      <td>0.144930</td>\n      <td>0.064646</td>\n      <td>0.028169</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.207300</td>\n      <td>0.169695</td>\n      <td>0.139272</td>\n      <td>0.063043</td>\n      <td>0.028169</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.207400</td>\n      <td>0.168045</td>\n      <td>0.137274</td>\n      <td>0.061232</td>\n      <td>0.032193</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n[DEBUG compute_metrics] Raw preds_ids (first example, up to 50 tokens): [    0     3    15   566     3  8637  5973    44     8 20052  1121    13\n     8   868 10639    13 12263    41   262   189 24362   157    32    37\n    32   144    52    32     3    61    11   263   112     3     7  2408\n    63  5695    16 25745     3     6 16069    38     8 11595    16  4599\n   451  1639]\n[DEBUG compute_metrics] Shape of preds_ids: (497, 128)\n[DEBUG compute_metrics] Min ID in preds: -100, Max ID in preds: 31968\n[DEBUG compute_metrics] Tokenizer vocab size: 32000\n\n[DEBUG compute_metrics] Raw preds_ids (first example, up to 50 tokens): [    0   454     3  8637  5973    44     8 20052  1121    13     8   868\n 10639    13 12263    41   262   189 24362   157    32    37    32   144\n    52    32     3    61    11   263   112  1415  5695    16 25745     3\n     6 16069    38     8 11595    16  4599   451  1639     7   855   355\n     3    31]\n[DEBUG compute_metrics] Shape of preds_ids: (497, 97)\n[DEBUG compute_metrics] Min ID in preds: -100, Max ID in preds: 31968\n[DEBUG compute_metrics] Tokenizer vocab size: 32000\n\n[DEBUG compute_metrics] Raw preds_ids (first example, up to 50 tokens): [    0   216     3  8637  5973    44     8 20052  1121    13     8   868\n 10639    13 12263    41   262   189 24362   157    32    37    32   144\n    52    32     3    61    11   263   112  1415  5695    16 25745     3\n     6 16069    38     8 11595    16  4599   451  1639     7   855   355\n     3    31]\n[DEBUG compute_metrics] Shape of preds_ids: (497, 95)\n[DEBUG compute_metrics] Min ID in preds: -100, Max ID in preds: 31968\n[DEBUG compute_metrics] Tokenizer vocab size: 32000\n\n[DEBUG compute_metrics] Raw preds_ids (first example, up to 50 tokens): [    0   216     3  8637  5973    44     8 20052  1121    13     8   868\n 10639    13 12263    41   262   189 24362   157    32    37    32   144\n    52    32     3    61    11   263   112  1415  5695    16 25745     3\n     6 16069    38     8 11595    16  4599   451  1639     7   855   355\n     3    31]\n[DEBUG compute_metrics] Shape of preds_ids: (497, 96)\n[DEBUG compute_metrics] Min ID in preds: -100, Max ID in preds: 31968\n[DEBUG compute_metrics] Tokenizer vocab size: 32000\n","output_type":"stream"},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Training finished successfully.\n***** train metrics *****\n  epoch                    =     1.9976\n  total_flos               =   313951GF\n  train_loss               =     0.2168\n  train_runtime            = 2:14:32.63\n  train_samples_per_second =      1.236\n  train_steps_per_second   =      0.154\nBest model and tokenizer saved to /kaggle/working/spell_correction_outputs/best_spell_corrector_model\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# --- 7. Evaluate the Performance on Test Dataset ---\nif 'can_train' in locals() and can_train and 'best_model_path' in locals() : # Check if training was attempted and successful\n    print(f\"\\nEvaluating the fine-tuned model on the test set...\")\n    if tokenized_test_dataset_for_eval and len(tokenized_test_dataset_for_eval) > 0:\n        try:\n            test_results = trainer.evaluate(\n                eval_dataset=tokenized_test_dataset_for_eval,\n                metric_key_prefix=\"test\" # Adds \"test_\" prefix to metric names\n            )\n            print(\"\\nTest Set Evaluation Results:\")\n            for key, value in test_results.items():\n                # Ensure value is float for consistent formatting, handle potential None\n                value_to_print = value if isinstance(value, (int, float)) else 0.0\n                print(f\"  {key}: {value_to_print:.4f}\")\n            \n            # Save test metrics\n            trainer.log_metrics(\"test\", test_results)\n            trainer.save_metrics(\"test\", test_results)\n        except Exception as e:\n            print(f\"An error occurred during test set evaluation: {e}\")\n            import traceback\n            traceback.print_exc()\n    else:\n        print(\"Test dataset (tokenized_test_dataset_for_eval) is empty. Skipping test set evaluation.\")\nelse:\n    print(\"\\nSkipping test set evaluation as model training was not performed or did not complete successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T03:02:28.369229Z","iopub.execute_input":"2025-05-14T03:02:28.369593Z","iopub.status.idle":"2025-05-14T03:07:58.831449Z","shell.execute_reply.started":"2025-05-14T03:02:28.369562Z","shell.execute_reply":"2025-05-14T03:07:58.830263Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating the fine-tuned model on the test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 05:15]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n[DEBUG compute_metrics] Raw preds_ids (first example, up to 50 tokens): [    0   216 17785    12 20134     7  3763    16  2038    11  1632 12130\n 17418    21     8 25491     7    16  1421     3     5     1     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0]\n[DEBUG compute_metrics] Shape of preds_ids: (500, 107)\n[DEBUG compute_metrics] Min ID in preds: -100, Max ID in preds: 31993\n[DEBUG compute_metrics] Tokenizer vocab size: 32000\n","output_type":"stream"},{"name":"stderr","text":"early stopping required metric_for_best_model, but did not find eval_cer so early stopping is disabled\n","output_type":"stream"},{"name":"stdout","text":"\nTest Set Evaluation Results:\n  test_loss: 0.1753\n  test_wer: 0.1396\n  test_cer: 0.0643\n  test_exact_match_accuracy: 0.0240\n  test_runtime: 330.4428\n  test_samples_per_second: 1.5130\n  test_steps_per_second: 0.1910\n  epoch: 1.9976\n***** test metrics *****\n  epoch                     =     1.9976\n  test_cer                  =     0.0643\n  test_exact_match_accuracy =      0.024\n  test_loss                 =     0.1753\n  test_runtime              = 0:05:30.44\n  test_samples_per_second   =      1.513\n  test_steps_per_second     =      0.191\n  test_wer                  =     0.1396\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# --- Inference Function & Demonstration ---\n\n# Determine which model and tokenizer to use for inference\nmodel_for_inference = None\ntokenizer_for_inference = None\nbest_model_path_to_load = os.path.join(OUTPUT_DIR, \"best_spell_corrector_model\") # Path where model was saved\n\nif 'can_train' in locals() and can_train and os.path.exists(best_model_path_to_load):\n    print(f\"\\nLoading best fine-tuned model from {best_model_path_to_load} for inference...\")\n    try:\n        model_for_inference = T5ForConditionalGeneration.from_pretrained(best_model_path_to_load)\n        tokenizer_for_inference = T5Tokenizer.from_pretrained(best_model_path_to_load)\n        print(\"Successfully loaded fine-tuned model and tokenizer.\")\n    except Exception as e:\n        print(f\"Error loading fine-tuned model from {best_model_path_to_load}: {e}. Falling back to base model.\")\n        model_for_inference = None # Ensure fallback\n\nif model_for_inference is None: # Fallback if fine-tuned model loading failed or training was skipped\n    print(f\"\\nFine-tuned model not available. Using base {MODEL_NAME} for inference (results may be indicative only).\")\n    model_for_inference = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n    tokenizer_for_inference = T5Tokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n\nmodel_for_inference.to(device)\nmodel_for_inference.eval() # Set to evaluation mode (disables dropout etc.)\nprint(f\"Model for inference is on device: {model_for_inference.device}\")\n\n\ndef correct_spelling_batch(text_list, max_correction_len=MAX_TARGET_LENGTH):\n    \"\"\"Corrects spelling for a batch of input texts.\"\"\"\n    if not isinstance(text_list, list):\n        text_list = [str(text_list)] # Ensure it's a list of strings\n    else:\n        text_list = [str(text) for text in text_list]\n\n    if not text_list:\n        return []\n    \n    # Add T5 prefix\n    prefixed_texts = [\"correct spelling: \" + text.strip().replace(\"\\n\",\" \") for text in text_list]\n    \n    # Tokenize\n    inputs = tokenizer_for_inference(\n        prefixed_texts,\n        return_tensors=\"pt\",    # PyTorch tensors\n        padding=True,           # Pad to longest in batch\n        truncation=True,        # Truncate if too long\n        max_length=MAX_SOURCE_LENGTH\n    ).to(device) # Move inputs to the same device as the model\n    \n    # Generate corrected sequences\n    with torch.no_grad(): # Disable gradient calculations for inference\n        summary_ids = model_for_inference.generate(\n            inputs['input_ids'],\n            attention_mask=inputs['attention_mask'], # Pass attention mask\n            num_beams=4,                             # Beam search for potentially better quality\n            no_repeat_ngram_size=2,                  # Helps prevent repetitive phrases\n            min_length=max(1, int(MAX_TARGET_LENGTH * 0.1)), # Avoid overly short trivial outputs\n            max_length=max_correction_len,\n            early_stopping=True                      # Stop when EOS token is generated by all beams\n        )\n    \n    # Decode generated token IDs back to text\n    corrected_texts = tokenizer_for_inference.batch_decode(summary_ids, skip_special_tokens=True)\n    return corrected_texts\n\n\n# --- Demonstration using examples from the original test_data_df ---\nprint(\"\\n--- Demonstrating Spelling Correction on Test Examples ---\")\n\n# test_data_df is the pandas DataFrame with 'misspelled' and 'correct' columns created in Cell 4\nif 'test_data_df' in locals() and not test_data_df.empty:\n    # Take a few random samples for demonstration\n    num_demo_samples = min(5, len(test_data_df))\n    demo_samples_df = test_data_df.sample(n=num_demo_samples, random_state=77) # Use a fixed seed for demo\n\n    for _, row in demo_samples_df.iterrows():\n        misspelled_text = row['misspelled']\n        original_correct_text = row['correct']\n        \n        print(f\"\\nOriginal Misspelled : {misspelled_text}\")\n        print(f\"Ground Truth Correct: {original_correct_text}\")\n        \n        # Correct spelling (pass as a list, even if it's one sentence)\n        corrected_output_list = correct_spelling_batch([misspelled_text])\n        \n        if corrected_output_list:\n            model_corrected_text = corrected_output_list[0]\n            print(f\"Model Corrected     : {model_corrected_text}\")\n            \n            # Calculate edit distance (case-insensitive comparison)\n            if original_correct_text and model_corrected_text:\n                try:\n                    dist = edit_distance(model_corrected_text.lower().strip(), original_correct_text.lower().strip())\n                    print(f\"Edit Distance (Model vs Truth, lower is better): {dist}\")\n                except Exception as e_dist:\n                    print(f\"Could not calculate edit distance: {e_dist}\")\n        else:\n            print(f\"Model Corrected     : [NO OUTPUT OR ERROR]\")\n            \nelse:\n    print(\"Original test data (test_data_df) not available or empty. Skipping demonstration on test examples.\")\n\n# Example of correcting a new, custom sentence\nprint(\"\\n--- Correcting a New Custom Sentence ---\")\ncustom_misspelled_sentence = \"Ths is a sentance wth sme speling errrs and I hope it gets fxed.\"\nprint(f\"Input Misspelled: {custom_misspelled_sentence}\")\n\ncorrected_custom_list = correct_spelling_batch([custom_misspelled_sentence])\nif corrected_custom_list:\n    print(f\"Model Corrected : {corrected_custom_list[0]}\")\nelse:\n    print(f\"Model Corrected : [NO OUTPUT OR ERROR]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T03:07:58.832525Z","iopub.execute_input":"2025-05-14T03:07:58.832813Z","iopub.status.idle":"2025-05-14T03:08:07.979970Z","shell.execute_reply.started":"2025-05-14T03:07:58.832791Z","shell.execute_reply":"2025-05-14T03:08:07.978524Z"}},"outputs":[{"name":"stdout","text":"\nLoading best fine-tuned model from /kaggle/working/spell_correction_outputs/best_spell_corrector_model for inference...\nSuccessfully loaded fine-tuned model and tokenizer.\nModel for inference is on device: cpu\n\n--- Demonstrating Spelling Correction on Test Examples ---\n\nOriginal Misspelled : Produced by Warner Beros. Pictures , the film premiered in New York Cityy on December 13 , 2016 amd had a limited release oq Decembepr 25 , 2016 , and had a wide release on January 13 , 2017 .\nGround Truth Correct: Produced by Warner Bros. Pictures , the film premiered in New York City on December 13 , 2016 and had a limited release on December 25 , 2016 , and had a wide release on January 13 , 2017 .\nModel Corrected     : Produced by Warner Bros. Pictures, the film premiered in New York City on December 13. 2016 and had a limited release on Decembepr 25 - 2016 ; and was able to open on January 13 and 2017!\nEdit Distance (Model vs Truth, lower is better): 24\n\nOriginal Misspelled : Various elements and obstacles are introduced aas one moves on to new levesl , which means that the complexity acd level of puzzle solving rquired ogradually increases as they progress throughh the gme .\nGround Truth Correct: Various elements and obstacles are introduced as one moves on to new levels , which means that the complexity and level of puzzle solving required gradually increases as they progress through the game .\nModel Corrected     : Various elements and obstacles are introduced as one moves on to new areas, which means that the complexity of the level of puzzle solving required gradually increases as they progress through the process.\nEdit Distance (Model vs Truth, lower is better): 17\n\nOriginal Misspelled : Hoowever , thhe original ylag hazd a brownish oclour instead of green , atnd there ae horizontal variants of the flag aas well .\nGround Truth Correct: However , the original flag had a brownish colour instead of green , and there are horizontal variants of the flag as well .\nModel Corrected     : Hoowever, the original ylag hazd a brownish clour instead of green and there were horizontal variants of the flag as well.\nEdit Distance (Model vs Truth, lower is better): 10\n\nOriginal Misspelled : Khurramm tries to return back to the camp from tthe battle but takes longv tmie to reach as he forgets his way and Mumatz Mahal diesy while giving birth to her nineteenth hild .\nGround Truth Correct: Khurram tries to return back to the camp from the battle but takes long time to reach as he forgets his way and Mumtaz Mahal dies while giving birth to her nineteenth child .\nModel Corrected     : Khurramm tries to return back to the camp from the battle but takes long to reach as he forgets his way and Mumatz Mahal dies while giving birth to her nineteenth birthday.\nEdit Distance (Model vs Truth, lower is better): 15\n\nOriginal Misspelled : '' Bellringer '' was in fact a derivativqe fo '' Hellbringer , '' a nickname giyen to him vy fellow musician Dan Massie rn rfeerence to hhis unquenchable thirst for debaucheyr and outlandish clsthing .\nGround Truth Correct: '' Bellringer '' was in fact a derivative of '' Hellbringer , '' a nickname given to him by fellow musician Dan Massie in reference to his unquenchable thirst for debauchery and outlandish clothing .\nModel Corrected     : \"' Bellringer '' was in fact a derivativist for \" \" Hellbringer, \"' an nickname given to him by fellow musician Dan Massie to his unquenchable thirst for debauchey and outlandishthing.\nEdit Distance (Model vs Truth, lower is better): 31\n\n--- Correcting a New Custom Sentence ---\nInput Misspelled: Ths is a sentance wth sme speling errrs and I hope it gets fxed.\nModel Corrected : Ths is a sentance for the speling errors and I hope it gets done.\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}