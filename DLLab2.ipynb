{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11762848,"sourceType":"datasetVersion","datasetId":7384614},{"sourceId":11800828,"sourceType":"datasetVersion","datasetId":7410814}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"!pip install jiwer","metadata":{}},{"cell_type":"code","source":"!pip install evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:58:40.626292Z","iopub.execute_input":"2025-05-14T00:58:40.626991Z","iopub.status.idle":"2025-05-14T00:58:43.993819Z","shell.execute_reply.started":"2025-05-14T00:58:40.626964Z","shell.execute_reply":"2025-05-14T00:58:43.993047Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:58:43.998828Z","iopub.execute_input":"2025-05-14T00:58:43.999536Z","iopub.status.idle":"2025-05-14T00:58:46.948885Z","shell.execute_reply.started":"2025-05-14T00:58:43.999500Z","shell.execute_reply":"2025-05-14T00:58:46.948172Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (3.1.0)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\nRequirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.13.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq,\n    EarlyStoppingCallback\n)\nfrom datasets import Dataset as HFDataset\nimport nltk\nfrom nltk.metrics.distance import edit_distance\nfrom sklearn.model_selection import train_test_split\nimport evaluate\nimport gc\nimport re\n\n# Download necessary nltk resources\nnltk.download('punkt')\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed_all(42)\n\n# Define paths\nDATA_DIR = \"/kaggle/input/transformer-mode-for-spelling-correction/wiki-split-master\"  # Default location for Kaggle dataset\nCACHE_DIR = \"/kaggle/working/cache\"  # For saving model checkpoints\nOUTPUT_DIR = \"/kaggle/working/outputs\"  # For saving outputs\n\n# Create cache and output directories if they don't exist\nos.makedirs(CACHE_DIR, exist_ok=True)\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:58:46.949887Z","iopub.execute_input":"2025-05-14T00:58:46.950172Z","iopub.status.idle":"2025-05-14T00:58:58.391172Z","shell.execute_reply.started":"2025-05-14T00:58:46.950114Z","shell.execute_reply":"2025-05-14T00:58:58.390369Z"}},"outputs":[{"name":"stderr","text":"2025-05-14 00:58:53.945011: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747184333.967350    1893 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747184333.974331    1893 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 1. Load the WikiSplit dataset\ndef load_data(file_path, max_samples=None):\n    print(f\"Loading data from {file_path}\")\n    df = pd.read_csv(file_path, sep='\\t', header=None, names=['correct', 'split'])\n    # Keep only the first column (correct sentences)\n    df = df[['correct']]\n    if max_samples:\n        df = df.sample(n=min(max_samples, len(df)), random_state=42)\n    return df\n\n# Set the maximum number of samples to use (for memory efficiency)\n# Using just 10% of the original sizes for extreme memory efficiency\nMAX_TRAIN_SAMPLES = 50000   # Reduced to 5k examples for training (10% of original)\nMAX_VAL_SAMPLES = 5000      # Reduced to 500 examples for validation\nMAX_TEST_SAMPLES = 5000     # Reduced to 500 examples for testing\n\n# Load the datasets (with limits for memory efficiency)\ntrain_df = load_data(os.path.join(DATA_DIR, \"tune.tsv\"), MAX_TRAIN_SAMPLES)\nval_df = load_data(os.path.join(DATA_DIR, \"validation.tsv\"), MAX_VAL_SAMPLES)\ntest_df = load_data(os.path.join(DATA_DIR, \"test.tsv\"), MAX_TEST_SAMPLES)\n\nprint(f\"Train samples: {len(train_df)}\")\nprint(f\"Validation samples: {len(val_df)}\")\nprint(f\"Test samples: {len(test_df)}\")\n\n# 2. Generate sentences with spelling mistakes\ndef generate_misspelled_sentence(sentence, error_prob=0.3, char_error_prob=0.7):\n    \"\"\"\n    Generate a misspelled sentence by introducing various types of spelling errors:\n    - Character swapping\n    - Character deletion\n    - Character insertion\n    - Character replacement\n    \n    Args:\n        sentence: Original sentence\n        error_prob: Probability of introducing an error to a word\n        char_error_prob: Probability of character-level error (vs word-level)\n        \n    Returns:\n        Misspelled sentence\n    \"\"\"\n    words = sentence.split()\n    misspelled_words = []\n    \n    for word in words:\n        # Skip very short words and punctuation\n        if len(word) <= 2 or not any(c.isalpha() for c in word):\n            misspelled_words.append(word)\n            continue\n            \n        # Decide whether to introduce an error to this word\n        if random.random() < error_prob:\n            # Character-level errors\n            if random.random() < char_error_prob:\n                chars = list(word)\n                error_type = random.choice(['swap', 'delete', 'insert', 'replace'])\n                \n                if error_type == 'swap' and len(word) >= 2:\n                    # Swap two adjacent characters\n                    idx = random.randint(0, len(chars) - 2)\n                    chars[idx], chars[idx + 1] = chars[idx + 1], chars[idx]\n                    \n                elif error_type == 'delete' and len(word) >= 3:\n                    # Delete a character\n                    idx = random.randint(0, len(chars) - 1)\n                    chars.pop(idx)\n                    \n                elif error_type == 'insert':\n                    # Insert a random character\n                    idx = random.randint(0, len(chars))\n                    chars.insert(idx, random.choice('abcdefghijklmnopqrstuvwxyz'))\n                    \n                elif error_type == 'replace' and len(word) >= 1:\n                    # Replace a character with another random character\n                    idx = random.randint(0, len(chars) - 1)\n                    new_char = random.choice('abcdefghijklmnopqrstuvwxyz')\n                    while new_char == chars[idx]:\n                        new_char = random.choice('abcdefghijklmnopqrstuvwxyz')\n                    chars[idx] = new_char\n                \n                misspelled_words.append(''.join(chars))\n            \n            # Word-level errors (split or merge words)\n            else:\n                error_type = random.choice(['split', 'merge'])\n                \n                if error_type == 'split' and len(word) >= 4:\n                    # Split the word into two parts\n                    split_idx = random.randint(1, len(word) - 1)\n                    misspelled_words.append(word[:split_idx])\n                    misspelled_words.append(word[split_idx:])\n                \n                elif error_type == 'merge' and len(misspelled_words) > 0 and len(misspelled_words[-1]) + len(word) <= 15:\n                    # Merge with previous word (with some constraints)\n                    prev_word = misspelled_words.pop()\n                    misspelled_words.append(prev_word + word)\n\n                else:\n                    # Default to adding the word as is if merge isn't possible\n                    misspelled_words.append(word)\n        else:\n            misspelled_words.append(word)\n    \n    return ' '.join(misspelled_words)\n\n# Create a function to generate a dataset with spelling mistakes\ndef create_spelling_dataset(df, num_variants=1):\n    \"\"\"\n    Create a dataset with spelling mistakes\n    \n    Args:\n        df: DataFrame with correct sentences\n        num_variants: Number of misspelled variants to generate per sentence\n        \n    Returns:\n        DataFrame with misspelled and correct sentences\n    \"\"\"\n    misspelled_data = []\n    \n    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating misspelled sentences\"):\n        correct = row['correct']\n        \n        # Skip sentences that are too long (for memory efficiency)\n        if len(correct.split()) > 30:\n            continue\n            \n        for _ in range(num_variants):\n            misspelled = generate_misspelled_sentence(correct)\n            misspelled_data.append({\n                'misspelled': misspelled,\n                'correct': correct\n            })\n    \n    return pd.DataFrame(misspelled_data)\n\n# Generate datasets with spelling mistakes\nprint(\"Generating datasets with spelling mistakes...\")\ntrain_data = create_spelling_dataset(train_df)\nval_data = create_spelling_dataset(val_df)\ntest_data = create_spelling_dataset(test_df)\n\nprint(f\"Generated train samples: {len(train_data)}\")\nprint(f\"Generated validation samples: {len(val_data)}\")\nprint(f\"Generated test samples: {len(test_data)}\")\n\n# Let's check a few examples to confirm our spelling mistakes generation\nprint(\"\\nSample data (first 3 examples):\")\nfor i in range(min(3, len(train_data))):\n    print(f\"Original  : {train_data.iloc[i]['correct']}\")\n    print(f\"Misspelled: {train_data.iloc[i]['misspelled']}\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:58:58.392207Z","iopub.execute_input":"2025-05-14T00:58:58.392828Z","iopub.status.idle":"2025-05-14T00:58:59.359831Z","shell.execute_reply.started":"2025-05-14T00:58:58.392808Z","shell.execute_reply":"2025-05-14T00:58:59.359001Z"}},"outputs":[{"name":"stdout","text":"Loading data from /kaggle/input/transformer-mode-for-spelling-correction/wiki-split-master/tune.tsv\nLoading data from /kaggle/input/transformer-mode-for-spelling-correction/wiki-split-master/validation.tsv\nLoading data from /kaggle/input/transformer-mode-for-spelling-correction/wiki-split-master/test.tsv\nTrain samples: 5000\nValidation samples: 5000\nTest samples: 5000\nGenerating datasets with spelling mistakes...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating misspelled sentences:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4160a513479d47ee8580cb25b6ef9fc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating misspelled sentences:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6758492838941e5a8a8c6507e074e1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating misspelled sentences:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3faa50d265f4a538daa315826bc9d44"}},"metadata":{}},{"name":"stdout","text":"Generated train samples: 1962\nGenerated validation samples: 1970\nGenerated test samples: 1876\n\nSample data (first 3 examples):\nOriginal  : He lived here with his wife Maria Hansen , the daughter of the Danish / German astronomer Peter Andreas Hansen .\nMisspelled: He lived hre with his wife aMria Hansne , ohe daughter of theDanish / German astronomer Petter Andrdas Hansen .\n\nOriginal  : Many groups want the cross to be included , while other organizations , notably American Atheists , disagree .\nMisspelled: Many groups want the cross to be included , while hother organizations , notably American Atheists , dtisagree .\n\nOriginal  : By 1759 , the Dutch East India Company 's trade had fallen substantially , and operations were moved to Bombay , with Suratte playing a subordinate role .\nMisspelled: By 1759 , the Dutch Easth India Company 's trade hadifallen suistantially ,and operations were moved to Bombay , with uSratte playcng a subordinate role .\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# 3. Create datasets in HuggingFace format\ndef create_hf_dataset(df):\n    return HFDataset.from_pandas(df)\n\ntrain_dataset = create_hf_dataset(train_data)\nval_dataset = create_hf_dataset(val_data)\ntest_dataset = create_hf_dataset(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:58:59.360784Z","iopub.execute_input":"2025-05-14T00:58:59.360986Z","iopub.status.idle":"2025-05-14T00:58:59.384759Z","shell.execute_reply.started":"2025-05-14T00:58:59.360971Z","shell.execute_reply":"2025-05-14T00:58:59.384252Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 4. Preprocess data for the model\n# We'll use T5 for this task (smaller than BART, better for memory constraints)\n# T5-small is a good choice for Kaggle's T4x2 environment\nMODEL_NAME = \"t5-base\"\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n\n# Set max length based on our dataset analysis (adjust if needed)\nMAX_SOURCE_LENGTH = 64   # Reduced from 128\nMAX_TARGET_LENGTH = 64   # Reduced from 128\n\ndef preprocess_function(examples):\n    \"\"\"Tokenize the texts and prepare them for the model\"\"\"\n    # For T5, we need to add a prefix for the task\n    inputs = [\"correct spelling: \" + ex for ex in examples[\"misspelled\"]]\n    targets = examples[\"correct\"]\n    \n    model_inputs = tokenizer(\n        inputs, \n        max_length=MAX_SOURCE_LENGTH, \n        padding=\"max_length\", \n        truncation=True\n    )\n    \n    # Setup the tokenizer for targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets,\n            max_length=MAX_TARGET_LENGTH,\n            padding=\"max_length\", \n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply preprocessing to all datasets\nprint(\"Tokenizing datasets...\")\ntrain_dataset = train_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=train_dataset.column_names,\n    desc=\"Preprocessing train dataset\",\n)\n\nval_dataset = val_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=val_dataset.column_names,\n    desc=\"Preprocessing validation dataset\",\n)\n\ntest_dataset = test_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=test_dataset.column_names,\n    desc=\"Preprocessing test dataset\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:58:59.385449Z","iopub.execute_input":"2025-05-14T00:58:59.385718Z","iopub.status.idle":"2025-05-14T00:59:02.203052Z","shell.execute_reply.started":"2025-05-14T00:58:59.385693Z","shell.execute_reply":"2025-05-14T00:59:02.202485Z"}},"outputs":[{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"name":"stdout","text":"Tokenizing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Preprocessing train dataset:   0%|          | 0/1962 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2544084218ac4a949d2adaf0f7df9c89"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Preprocessing validation dataset:   0%|          | 0/1970 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cb082113e6243a39749f0c50ecd9901"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Preprocessing test dataset:   0%|          | 0/1876 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4763d31ddaf94782acc3201825cea4d0"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# 5. Define Evaluation Metrics\nwer_metric = evaluate.load(\"wer\")\ncer_metric = evaluate.load(\"cer\")\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    \n    # Replace -100 in labels with the pad token id for decoding\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    # Decode predictions and labels\n    # Make decoding more robust to potentially invalid token IDs from a poorly trained model\n    \n    decoded_preds = []\n    for pred_seq in preds:\n        if isinstance(pred_seq, (np.ndarray, torch.Tensor)):\n            pred_seq_list = pred_seq.tolist()\n        else:\n            pred_seq_list = list(pred_seq) # Ensure it's a list\n\n        valid_pred_tokens = []\n        for token_id in pred_seq_list:\n            if isinstance(token_id, int) and 0 <= token_id < tokenizer.vocab_size:\n                valid_pred_tokens.append(token_id)\n            else:\n                pass\n\n        if not valid_pred_tokens: # If all tokens were invalid, or sequence became empty\n            decoded_preds.append(\"\")\n        else:\n            try:\n                decoded_preds.append(tokenizer.decode(valid_pred_tokens, skip_special_tokens=True))\n            except IndexError: \n                decoded_preds.append(\"\") \n                \n    # Decode labels (usually less problematic)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    \n    decoded_labels = [label if label else \"<empty>\" for label in decoded_labels]\n    decoded_preds = [pred if pred else \"<empty_pred>\" for pred in decoded_preds]\n\n\n    # Calculate Word Error Rate (WER)\n    try:\n        wer = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n    except Exception as e:\n        print(f\"Error computing WER: {e}. Decoded preds: {decoded_preds[:3]}, Decoded labels: {decoded_labels[:3]}\")\n        wer = 1.0 # Assign a bad score\n\n    # Calculate Character Error Rate (CER)\n    try:\n        cer = cer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n    except Exception as e:\n        print(f\"Error computing CER: {e}. Decoded preds: {decoded_preds[:3]}, Decoded labels: {decoded_labels[:3]}\")\n        cer = 1.0 # Assign a bad score\n    \n    # Calculate exact match accuracy\n    exact_matches = sum([1 if pred == label else 0 for pred, label in zip(decoded_preds, decoded_labels)])\n    exact_match_accuracy = exact_matches / len(decoded_preds) if len(decoded_preds) > 0 else 0.0\n    \n    return {\n        \"wer\": wer,\n        \"cer\": cer,\n        \"exact_match\": exact_match_accuracy\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:59:02.203819Z","iopub.execute_input":"2025-05-14T00:59:02.204076Z","iopub.status.idle":"2025-05-14T00:59:02.710931Z","shell.execute_reply.started":"2025-05-14T00:59:02.204057Z","shell.execute_reply":"2025-05-14T00:59:02.710174Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 6. Load and configure the model\nprint(f\"Loading {MODEL_NAME} model...\")\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n\nmodel.to(device)  # Move model to GPU if available\n\n# Print model size for reference\nmodel_size = sum(p.numel() for p in model.parameters()) / 1_000_000\nprint(f\"Model size: {model_size:.2f}M parameters\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:59:02.711748Z","iopub.execute_input":"2025-05-14T00:59:02.711958Z","iopub.status.idle":"2025-05-14T00:59:03.761644Z","shell.execute_reply.started":"2025-05-14T00:59:02.711941Z","shell.execute_reply":"2025-05-14T00:59:03.760797Z"}},"outputs":[{"name":"stdout","text":"Loading t5-base model...\nModel size: 222.90M parameters\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 7. Setup training arguments\n# For extreme memory efficiency on Kaggle T4x2, use these settings:\nbatch_size = 8  # Reduced from 16\ngradient_accumulation_steps = 2  # Increased from 2\n\n# Disable wandb to avoid initialization issues\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    eval_strategy=\"steps\",\n    eval_steps=50,  # Evaluate more frequently since we have less data\n    save_strategy=\"steps\",\n    save_steps=50,  # Save more frequently\n    save_total_limit=1,  # Keep only the 1 most recent checkpoint to save space\n    learning_rate=5e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    num_train_epochs=5,\n    predict_with_generate=True,\n    generation_max_length=MAX_TARGET_LENGTH,\n    fp16=True,  # Use mixed precision for memory efficiency\n    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n    logging_steps=20,  # Log more frequently\n    load_best_model_at_end=True,\n    metric_for_best_model=\"cer\",  # Use CER as the metric to track for best model\n    greater_is_better=False,  # Lower CER is better\n    remove_unused_columns=True,\n    dataloader_pin_memory=False,  # Disable to save memory\n    optim=\"adamw_torch\",  # Use Adafactor optimizer instead of AdamW (uses less memory)\n    report_to=[\"none\"],  # Disable all reporting to save memory and avoid issues\n)\n\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    label_pad_token_id=-100,\n    pad_to_multiple_of=8 if training_args.fp16 else None\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:59:03.762788Z","iopub.execute_input":"2025-05-14T00:59:03.763387Z","iopub.status.idle":"2025-05-14T00:59:03.798983Z","shell.execute_reply.started":"2025-05-14T00:59:03.763364Z","shell.execute_reply":"2025-05-14T00:59:03.798425Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# 8. Initialize trainer with early stopping to prevent overfitting\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\n# Add debugging info before training\nprint(\"\\nVerifying datasets before training:\")\nprint(f\"Training examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")\nprint(f\"Test examples: {len(test_dataset)}\")\n\n# Check one example from training data\nexample = train_dataset[0]\nprint(\"\\nExample input features:\")\nprint(f\"- input_ids shape: {len(example['input_ids'])}\")\nprint(f\"- attention_mask shape: {len(example['attention_mask'])}\")\nprint(f\"- labels shape: {len(example['labels'])}\")\n\n# Free some memory before training\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:59:03.799825Z","iopub.execute_input":"2025-05-14T00:59:03.800214Z","iopub.status.idle":"2025-05-14T00:59:04.176213Z","shell.execute_reply.started":"2025-05-14T00:59:03.800188Z","shell.execute_reply":"2025-05-14T00:59:04.175491Z"}},"outputs":[{"name":"stdout","text":"\nVerifying datasets before training:\nTraining examples: 1962\nValidation examples: 1970\nTest examples: 1876\n\nExample input features:\n- input_ids shape: 64\n- attention_mask shape: 64\n- labels shape: 64\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_1893/1885090836.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# 9. Train the model\nprint(\"\\nStarting model training with ultra-light settings...\")\ntrainer.train()\n\n# Clear GPU memory\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:59:04.178767Z","iopub.execute_input":"2025-05-14T00:59:04.179113Z","iopub.status.idle":"2025-05-14T01:20:29.286146Z","shell.execute_reply.started":"2025-05-14T00:59:04.179095Z","shell.execute_reply":"2025-05-14T01:20:29.285514Z"}},"outputs":[{"name":"stdout","text":"\nStarting model training with ultra-light settings...\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='305' max='305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [305/305 21:22, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Wer</th>\n      <th>Cer</th>\n      <th>Exact Match</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.600400</td>\n      <td>0.218844</td>\n      <td>0.154222</td>\n      <td>0.083996</td>\n      <td>0.085787</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.235000</td>\n      <td>0.173562</td>\n      <td>0.101448</td>\n      <td>0.054257</td>\n      <td>0.143147</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.187500</td>\n      <td>0.161038</td>\n      <td>0.093541</td>\n      <td>0.050915</td>\n      <td>0.164975</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.176700</td>\n      <td>0.156504</td>\n      <td>0.089046</td>\n      <td>0.047675</td>\n      <td>0.171574</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.176300</td>\n      <td>0.153439</td>\n      <td>0.087090</td>\n      <td>0.047812</td>\n      <td>0.177665</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.161700</td>\n      <td>0.152846</td>\n      <td>0.086861</td>\n      <td>0.047189</td>\n      <td>0.179695</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# 10. Evaluate the model on the test set\nprint(\"Evaluating model on test set...\")\ntest_results = trainer.evaluate(test_dataset)\nprint(f\"Test results: {test_results}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T01:20:29.286919Z","iopub.execute_input":"2025-05-14T01:20:29.287231Z","iopub.status.idle":"2025-05-14T01:22:59.339215Z","shell.execute_reply.started":"2025-05-14T01:20:29.287199Z","shell.execute_reply":"2025-05-14T01:22:59.338563Z"}},"outputs":[{"name":"stdout","text":"Evaluating model on test set...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='118' max='118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [118/118 02:12]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Test results: {'eval_loss': 0.1557590812444687, 'eval_wer': 0.08841262157285569, 'eval_cer': 0.04734651833539349, 'eval_exact_match': 0.18550106609808104, 'eval_runtime': 150.032, 'eval_samples_per_second': 12.504, 'eval_steps_per_second': 0.786, 'epoch': 4.926829268292683}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# 11. Save the model\ntrainer.save_model(os.path.join(OUTPUT_DIR, \"best_model\"))\nprint(f\"Best model saved to {os.path.join(OUTPUT_DIR, 'best_model')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T01:22:59.340174Z","iopub.execute_input":"2025-05-14T01:22:59.340463Z","iopub.status.idle":"2025-05-14T01:23:01.545899Z","shell.execute_reply.started":"2025-05-14T01:22:59.340434Z","shell.execute_reply":"2025-05-14T01:23:01.545188Z"}},"outputs":[{"name":"stdout","text":"Best model saved to /kaggle/working/outputs/best_model\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(\"Manually loading the last checkpoint...\")\n# List contents of output_dir to find the last checkpoint\ncheckpoint_folders = [f for f in os.listdir(OUTPUT_DIR) if f.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(OUTPUT_DIR, f))]\nif checkpoint_folders:\n    # Sort them to find the one with the highest step number\n    checkpoint_folders.sort(key=lambda x: int(x.split('-')[1]))\n    latest_checkpoint_dir = os.path.join(OUTPUT_DIR, checkpoint_folders[-1])\n    print(f\"Found latest checkpoint: {latest_checkpoint_dir}\")\n    \n    # Load the model from this checkpoint\n    # Make sure 'model' variable is reassigned\n    try:\n        model = T5ForConditionalGeneration.from_pretrained(latest_checkpoint_dir)\n        model.to(device) # Move to device again\n        print(\"Model successfully loaded manually from the latest checkpoint.\")\n        # If you also saved tokenizer and config, you might load them too, but usually not needed if tokenizer is already loaded.\n    except Exception as e:\n        print(f\"Error loading model manually: {e}\")\n        # This will show if the checkpoint itself is corrupted or has missing keys\nelse:\n    print(\"No checkpoint folders found in output directory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T01:23:01.546815Z","iopub.execute_input":"2025-05-14T01:23:01.547064Z","iopub.status.idle":"2025-05-14T01:23:02.153646Z","shell.execute_reply.started":"2025-05-14T01:23:01.547042Z","shell.execute_reply":"2025-05-14T01:23:02.152846Z"}},"outputs":[{"name":"stdout","text":"Manually loading the last checkpoint...\nFound latest checkpoint: /kaggle/working/outputs/checkpoint-300\nModel successfully loaded manually from the latest checkpoint.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# 12. Function to correct spelling of new sentences\ndef correct_spelling(sentences):\n    inputs = [\"correct spelling: \" + text for text in sentences]\n    inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SOURCE_LENGTH)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    outputs = model.generate(\n        **inputs,\n        max_length=MAX_TARGET_LENGTH,\n        num_beams=4,\n        early_stopping=True\n    )\n    \n    corrected = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    return corrected","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T01:23:02.154472Z","iopub.execute_input":"2025-05-14T01:23:02.154747Z","iopub.status.idle":"2025-05-14T01:23:02.159710Z","shell.execute_reply.started":"2025-05-14T01:23:02.154723Z","shell.execute_reply":"2025-05-14T01:23:02.158951Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def correct_spelling(sentences):\n\n    print(f\"Input to correction function: {sentences[:2]}\")\n    \n    inputs = [\"correct spelling: \" + text for text in sentences]\n    \n    # Tokenize\n    input_tokenized = tokenizer(\n        inputs, \n        return_tensors=\"pt\", \n        padding=True, \n        truncation=True, \n        max_length=MAX_SOURCE_LENGTH\n    )\n    \n    input_tokenized = {k: v.to(device) for k, v in input_tokenized.items()}\n    \n    print(f\"Input shapes: {input_tokenized['input_ids'].shape}\")\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_tokenized['input_ids'],\n            attention_mask=input_tokenized['attention_mask'],\n            max_length=MAX_TARGET_LENGTH,\n            num_beams=2,\n            early_stopping=True,\n            do_sample=False,  # Disable sampling for more deterministic output\n            length_penalty=1.0,  # Neutral length penalty\n            min_length=1,  # Ensure at least 1 token is generated\n            no_repeat_ngram_size=2  # Prevent repetition\n        )\n    \n    print(f\"Output shape: {outputs.shape}\")\n    print(f\"Raw output tokens: {outputs[0][:10].tolist()}\")  # Print first few tokens\n    \n    # Decode outputs with more visibility into the process\n    corrected = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    \n    # If still empty, try decoding without skipping special tokens\n    if all(not text for text in corrected):\n        print(\"Warning: Empty decoded outputs, trying alternative decoding...\")\n        corrected = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n        # Remove only the most common special tokens manually\n        corrected = [text.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").replace(\"<s>\", \"\").strip() \n                    for text in corrected]\n    \n    print(f\"Decoded outputs: {corrected[:2]}\")\n    \n    return corrected","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T01:23:02.160395Z","iopub.execute_input":"2025-05-14T01:23:02.160687Z","iopub.status.idle":"2025-05-14T01:23:02.174402Z","shell.execute_reply.started":"2025-05-14T01:23:02.160663Z","shell.execute_reply":"2025-05-14T01:23:02.173651Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"print(\"Pad token:\", tokenizer.pad_token)\nprint(\"EOS token:\", tokenizer.eos_token)\nprint(\"Decoder start token:\", tokenizer.pad_token_id)  # T5 often uses pad as decoder start","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T01:23:02.175261Z","iopub.execute_input":"2025-05-14T01:23:02.175579Z","iopub.status.idle":"2025-05-14T01:23:02.189454Z","shell.execute_reply.started":"2025-05-14T01:23:02.175562Z","shell.execute_reply":"2025-05-14T01:23:02.188717Z"}},"outputs":[{"name":"stdout","text":"Pad token: <pad>\nEOS token: </s>\nDecoder start token: 0\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# 13. Demonstrate the model on a few examples\nprint(\"\\nTesting the model on some examples:\")\ntest_examples = test_data.iloc[6:11].copy()\n\nprint(\"Original vs. Misspelled vs. Corrected:\")\nfor _, row in test_examples.iterrows():\n    misspelled = row['misspelled']\n    correct = row['correct']\n    \n    # Add try-except to catch any errors\n    try:\n        corrected = correct_spelling([misspelled])[0]\n    except Exception as e:\n        print(f\"Error during correction: {e}\")\n        corrected = \"\"  # Default to empty string on error\n    \n    print(f\"Misspelled: {misspelled}\")\n    print(f\"Correct   : {correct}\")\n    print(f\"Corrected : {corrected}\")\n    \n    # Calculate and show edit distance between corrected and correct versions\n    if corrected:\n        edit_dist = edit_distance(corrected, correct)\n        print(f\"Edit distance (corrected vs correct): {edit_dist}\\n\")\n    else:\n        print(\"Couldn't calculate edit distance (empty correction)\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T01:24:27.692202Z","iopub.execute_input":"2025-05-14T01:24:27.692550Z","iopub.status.idle":"2025-05-14T01:24:31.892936Z","shell.execute_reply.started":"2025-05-14T01:24:27.692524Z","shell.execute_reply":"2025-05-14T01:24:31.892169Z"}},"outputs":[{"name":"stdout","text":"\nTesting the model on some examples:\nOriginal vs. Misspelled vs. Corrected:\nInput to correction function: ['Due to my involvement inthe yachting industry , I am quihe knowledgeable in aisisting in thid industry as well as most other marine related information .']\nInput shapes: torch.Size([1, 41])\nOutput shape: torch.Size([1, 32])\nRaw output tokens: [0, 6984, 12, 82, 9683, 16, 8, 18082, 53, 681]\nDecoded outputs: ['Due to my involvement in the yachting industry , I am very knowledgeable in working in this industry as well as most other marine related information .']\nMisspelled: Due to my involvement inthe yachting industry , I am quihe knowledgeable in aisisting in thid industry as well as most other marine related information .\nCorrect   : Due to my involvement in the yachting industry , I am quite knowledgeable in assisting in this industry as well as most other marine related information .\nCorrected : Due to my involvement in the yachting industry , I am very knowledgeable in working in this industry as well as most other marine related information .\nEdit distance (corrected vs correct): 11\n\nInput to correction function: ['Contin uing no rth to northeast LA 27 enters DeiRdder as South Pine Street andx intersects with US 71/US 190 at teh northenr terinus .']\nInput shapes: torch.Size([1, 54])\nOutput shape: torch.Size([1, 37])\nRaw output tokens: [0, 3, 16798, 76, 53, 3457, 12, 25806, 5292, 2307]\nDecoded outputs: ['Continuing north to northeast LA 27 enters Downtown as South Pine Street and intersects with US 71/US 190 at the north end of downtown .']\nMisspelled: Contin uing no rth to northeast LA 27 enters DeiRdder as South Pine Street andx intersects with US 71/US 190 at teh northenr terinus .\nCorrect   : Continuing north to northeast LA 27 enters DeRidder as South Pine Street and intersects with US 171/US 190 at the northern terminus .\nCorrected : Continuing north to northeast LA 27 enters Downtown as South Pine Street and intersects with US 71/US 190 at the north end of downtown .\nEdit distance (corrected vs correct): 21\n\nInput to correction function: [\"Sutati Veerabhadra Rao ,popular as one of the famous '' Suti Jnta '' , was a popular Radio and TheateArtist from 1970 - 1980 .\"]\nInput shapes: torch.Size([1, 53])\nOutput shape: torch.Size([1, 50])\nRaw output tokens: [0, 1923, 7810, 3901, 15, 7093, 1024, 3515, 2922, 32]\nDecoded outputs: [\"Sutati Veerabhadra Rao ,popular as one of the famous '' Suti Jnta ’' ; was a popular Radio and Theate Artist from 1970 - 1980 .\"]\nMisspelled: Sutati Veerabhadra Rao ,popular as one of the famous '' Suti Jnta '' , was a popular Radio and TheateArtist from 1970 - 1980 .\nCorrect   : Sutti Veerabhadra Rao , popular as one of the famous '' Sutti Janta '' , was a popular Radio and Theater Artist from 1970 - 1980 .\nCorrected : Sutati Veerabhadra Rao ,popular as one of the famous '' Suti Jnta ’' ; was a popular Radio and Theate Artist from 1970 - 1980 .\nEdit distance (corrected vs correct): 7\n\nInput to correction function: [\"She is a popularsinger wh is co nsidered a idol in Japan during he 1970 's , and is thought to be representative of that era .\"]\nInput shapes: torch.Size([1, 44])\nOutput shape: torch.Size([1, 36])\nRaw output tokens: [0, 451, 19, 3, 9, 1012, 7634, 113, 19, 1702]\nDecoded outputs: [\"She is a popular singer who is considered  an idol in Japan during the 1970 's , and is thought to be representative of that era .\"]\nMisspelled: She is a popularsinger wh is co nsidered a idol in Japan during he 1970 's , and is thought to be representative of that era .\nCorrect   : She is a popular singer who is considered a idol in Japan during the 1970 's , and is thought to be representative of that era .\nCorrected : She is a popular singer who is considered  an idol in Japan during the 1970 's , and is thought to be representative of that era .\nEdit distance (corrected vs correct): 2\n\nInput to correction function: ['IMDb In the early 1990s , Frederik van Pa llandt settled in the Philippines where he and his Filipino girlfriend , Susannah , wezre shot dead in 1994 .']\nInput shapes: torch.Size([1, 49])\nOutput shape: torch.Size([1, 44])\nRaw output tokens: [0, 27, 11731, 115, 86, 8, 778, 5541, 7, 3]\nDecoded outputs: ['IMDb In the early 1990s , Frederik van Paaland settled in the Philippines where he and his Filipino girlfriend - Susannah   -- were shot dead in 1994 .']\nMisspelled: IMDb In the early 1990s , Frederik van Pa llandt settled in the Philippines where he and his Filipino girlfriend , Susannah , wezre shot dead in 1994 .\nCorrect   : IMDb In the early 1990s , Frederik van Pallandt settled in the Philippines where he and his Filipino girlfriend , Susannah , were shot dead in 1994 .\nCorrected : IMDb In the early 1990s , Frederik van Paaland settled in the Philippines where he and his Filipino girlfriend - Susannah   -- were shot dead in 1994 .\nEdit distance (corrected vs correct): 7\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}